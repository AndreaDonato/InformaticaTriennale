{
	"nodes":[
		{"id":"e723e0fbc1846d02","type":"text","text":"# Tipi di architetture\n\nPersonal computers (PCs) good performance to single users at low cost and usually\nexecute third-party software. 35 years old!\n\n\nServers usually accessed only via a network. oriented to carrying sizable\nworkloads, or handling many small jobs, e usually based on\nsoftware from another source (such as a database or simulation system), emphasis\non dependability, since a crash is usually  costly\n\nsupercomputers (configured as servers) which at the present consist of tens of thousands of processors and many terabytes of memory, weather forecasting, oil exploration, protein structure determination, and other large-scale problems\n\n\nEmbedded computers microprocessors networks of processors designed to run one application or one set of related applications that are normally integrated with the hardware and delivered as a single system; minimum performance with stringent limitations on cost or power lower tolerance for failure consumer-oriented embedded applications emphasis is on doing one function as perfectly as possible large embedded\nsystems redundancy often employed most concepts apply directly, or with slight modifications, to embedded computers.\n\npersonal mobile device (PMD) smart phone or a tablet tomorrow it may include  electronic glasses are small wireless devices to connect to the Internet\n\n\nCloud Computing large collections of servers that provide services over the Internet\n\n\nSoftware as a Service (SaaS) delivers software and data as a service over the Internet\nvia browser instead of binary code that must be installed runs wholly on that device e.g. web search social networking\n\n\n","x":-2440,"y":115,"width":720,"height":777,"color":"2"},
		{"id":"2452606adbf4ae6d","type":"file","file":"Capitolo 1/conversions.png","x":-2711,"y":-280,"width":991,"height":340},
		{"id":"70f63fa924d39f03","type":"text","text":"# Come influenza le performance?\n\n- Algorithm: number of source-level statements number of I/O operations executed\n- Programming language compiler, and architecture: number of computer instructions for each source-level statement\n- Processor and memory: how fast instructions can be executed\n- I/O system (hardware and OS): how fast I/O operations may be executed","x":-2160,"y":1612,"width":500,"height":377},
		{"id":"3c12a472e21d0b51","type":"text","text":"# Domande a cui rispondere\n\nHow are programs written in a high-level language translated into the language of the hardware, and how does the hardware\nexecute the resulting program?\n\nWhat is the interface between the software and the hardware, and how does software instruct the hardware to perform needed functions?\n\nhow can designers improve energy, efficiency, performance\n\nhow can programmer improve the performance\n \nswitch from sequential processing to parallel processing\n\nunderstanding these questions makes improving the performances a scientific procedure\n","x":-3431,"y":1091,"width":991,"height":398},
		{"id":"7c44d0b7fa4661bf","type":"text","text":"# Cuscinetti tra user e HW\n\nsystems software is the name of stuff sitting between the hardware and the application software. two types are central to every computer:\n- operating system: Supervising program that manages the resources of a computer for the benefit of the programs that run on that computer\n- compiler: translates high-level language statements into assembly language statements\n\nbut also\n\n-  loaders: loads programs in main memory\n- assemblers: translates a symbolic version of instructions into the binary version (assembly language A symbolic representation of machine instructions Assembly language requires the programmer to write one line for every instruction that the computer will follow, forcing the programmer to think like the computer)\n- ...\n\nthey provide\n- basic input and output\n- Allocating\n- sharing of the computer among multiple applications\n\n","x":-1020,"y":460,"width":991,"height":760},
		{"id":"3f606618d45d0104","type":"text","text":"# Tecniche di design\n\n- abstractions\n- make common case fast will tend to enhance performance better than optimizing the rare case\n- parallel performance\n- pipelining particular pattern of parallelism. runs programs faster by overlapping the execution of instructions industry has bet its future that programmers will switch to explicitly parallel programming.\n- prediction: it can be faster on average to guess and start working rather than wait until you know for sure\n- hierarchy of memories: memory speed often shapes performance, capacity limits the size of problems. cost of memory is majority of computer cost. hierarchy of memories: fastest, smallest, and the most expensive memory per bit at the top slowest, largest, and cheapest per bit at the bottom\n- Dependability via Redundancy: redundant components that can take over when a failure occurs and to help detect failures","x":-1910,"y":960,"width":720,"height":550},
		{"id":"f3d63dbf481dd171","type":"file","file":"Capitolo 1/ComputerScheme.png","x":300,"y":-356,"width":400,"height":333},
		{"id":"500dd90d04313081","type":"text","text":"# Commenti a caso che non so dove mettere (da qualche parte qui vicino)\n\nMoore’s Law integrated circuit resources double every 18–24 months computer designs can take years resources can easily double between the start and finish computer architects must anticipate where the technology will be when the design finishes  \n\n\n\nfive classic components of a computer are input, output, memory,\ndatapath, and control, with the last two sometimes combined and called\nthe processor\n\n\nExecution time is the only valid and unimpeachable measure of\nperformance. Many other metrics have been proposed and found wanting.\nSometimes these metrics are flawed from the start by not reflecting\nexecution time; other times a metric that is sound in a limited context\nis extended and used beyond that context or without the additional\nclarification needed to make it valid.\n\n\nmillion instructions per second (MIPS) A measurement of program execution speed based on the number of millions of instructions. \n$$\nMIPS = \\frac{I}{T_{cpu}*10^6} = \\frac{r_C}{CPI\\cdot 10^6}\n$$\n","x":-500,"y":-440,"width":720,"height":700},
		{"id":"94ebb589ac92b6e9","type":"text","text":"# Nomenclatura interna\n\n- integrated circuit (chip) dozens to millions of transistors.\n\n- CPU contains the datapath and control adds numbers, tests numbers, signals I/O devices to activate, and so on\n\t- Inside the processor is another type of memory, cache memory. small, fast memory that acts as a buffer for a slower, larger memory. SRAM (cache) faster and less dense than DRAM (RAM). cache is a safe place for hiding things (????)\n- datapath performs arithmetic operations\n- control: commands the datapath, memory, and I/O devices according to the instructions of the program.\n- DRAM: Memory built as an integrated circuit Access times are 50 nanoseconds In contrast to sequential access memories, such as magnetic tapes, the RAM portion of the term DRAM means that memory accesses take basically the same amount of time no matter what portion of the memory is read.\n- transistor: on/off switch controlled by an electric signal.\n- VLSI: device containing millions of transistors","x":200,"y":460,"width":460,"height":700},
		{"id":"46ee857cb70eed15","type":"text","text":"# Nomenclatura (concettuale) interna\n\ninterface between the hardware and the lowest-level software (**architecture** or ISA), includes anything programmers need to know to make a binary machine language program work correctly.\nallows computer designers to talk about functions\nindependently from the hardware\n\ncombination of the basic instruction set and the operating system interface provided for application programmers is called the application binary interface (ABI)\n\nimplementation of an architecture is hardware that obeys the architecture abstraction\n","x":700,"y":460,"width":440,"height":700},
		{"id":"969e5ce3a6049ee4","type":"text","text":"# Reti (cenni)\n\ncomputers on the network can share I/O devices ($\\Rightarrow$ Nonlocal access). Ethernet: up to a kilometer long and transfer at up to 40 gigabits per second.\n\nLAN network within a geographically\nconfined area, typically within a single building. capacity of from 1 to 40 gigabits per second\n\nWAN network that can span a continent\n\nOptical communications allowed growth in the capacity of wide area networks to gigabits and  to a worldwide network\n\nWireless technology enabled the post-PC era. ability to radiosignal with low-cost semiconductor technology (CMOS) used for memory and microprocessors enabled improvement in price","x":1200,"y":460,"width":420,"height":700},
		{"id":"6264c0d982a44972","type":"text","text":"## Giorgio vuole fare un breve trattato sull'Universo","x":1223,"y":120,"width":280,"height":140,"color":"1"},
		{"id":"0b6d09ee9d694b1f","type":"text","text":"# (Riassunto) 1 - Introduzione\n\nAl momento le frecce indicano l'ordine degli argomenti nel capitolo, non sono assolutamente utili in senso concettuale","x":-2480,"y":-550,"width":480,"height":220,"color":"6"},
		{"id":"7724895a9f26e30f","type":"text","text":"# Fabbricare un Chip\n\nmanufacture of a chip begins with silicon semiconductor. add materials to silicon that allow tiny areas to transform into\n\n- Excellent conductors (copper, aluminum)\n- Excellent insulators (plastic, glass)\n- conduct or insulate under specific conditions (transistor)\n\nTransistors fall into the last category process starts with silicon crystal ingot finely sliced into wafers then patterns of chemicals are placed on each wafer, creating the transistors, conductors, and insulators\n\nToday’s integrated circuits contain only one layer of transistors but may have from two to eight levels of metal conductor, separated by layers of insulators\n\nmicroscopic flaw in the wafer can result in failing. simplest way to cope with imperfection is to place many independent components on a single wafer\n\npatterned wafer is *diced*, Dicing enables you to discard only dies that contain the flaws ((**yield**: percentage of good dies))\n\nThe cost of an integrated circuit rises quickly as the die size increases\n$$\nCost\\,per\\,die = \\frac{Cost\\,per\\,wafer}{Dies\\,per\\,wafer\\times yield}\n$$\n$$\nDies\\,per\\,wafer) \\simeq \\frac{Wafer\\,area}{Die\\,area}\n$$\n\n$$\nyield = \\frac{1}{(1 + (Defects\\,per\\,area \\times Die\\,area/2))^2}\n$$\n\nThe final equation is based on empirical observations","x":220,"y":1320,"width":920,"height":1080},
		{"id":"e4506d54dcf914ea","type":"text","text":"# Performance\n\nwe can define computer performance in several distinct ways\n- **response time (execution time)**: As an individual computer user, the time between the start and completion of a task\n- Datacenters manage throughput or bandwidth (total amount of work done in a given time)\n\n##### Example: How to increase throughtput?\nmay be via (look for others...)\n- Replacing the processor in a computer with a faster version\n- also Adding additional processors to a system that uses multiple processors for separate tasks—for example, searching the web. If demand for processing in this second case is as large as the throughput, system might force requests to queue up increasing the throughput could also improve response time.\n\nwe could define\n$$\nperformance = \\frac{1}{execution\\,\\,time}\n$$\n\n###### Measuring performance\n\n- wall clock time, response time, or elapsed time: total time to complete a task.\n- processor may work on several programs simultaneously system may try to optimize throughput rather than elapsed time. **CPU execution time** is the time the CPU spends computing for this task and does not include time spent waiting for I/O or running other programs. further divided into\n\t- CPU time spent in the program (user CPU time)\n\t- CPU time spent in the operating system performing tasks on behalf of the program (system CPU time)\n\nDifferentiating between system and user is difficult.\n\nWe use \"system performance\" for elapsed time on an unloaded system and \"CPU performance\" for user CPU time. discussions of how to summarize performance can be applied to either elapsed time or CPU time measurements.\n\nMany applications depend as much on I/O performance, which relies on both hardware and software.\n\nTo improve the performance clear definition of what performance matters and find bottlenecks","x":1280,"y":1320,"width":817,"height":1080},
		{"id":"dfd2890763a0d962","type":"text","text":"# CPU\n\nclock determines when events take place in the hardware (so how fast the hardware can perform basic functions).\nDesigners refer to clock period both for complete clock cycle (picoseconds) and for the clock rate (GHz). Che cani.\n$$\nT_{cpu}(P) = N_{c}(P) \\cdot T_C = \\frac{N_{c}(P)}{r_c}\n$$\ndove $T_{CPU}(P)$ è CPU execution time for a Program P, $N_{c}(P)$ è CPU clock cycles for a given P, $T_c$ è Clock cycle time e $r_c$ è il suo inverso (clock rate). Quante ovvietà.\n\nImprove performance by reducing clock cycles required or the length of the clock cycle. Often trade-off: Many techniques that decrease the number of clock cycles may also increase the clock cycle time\n\nexecution time must depend on the number $I(P)$ of instructions executed in a program (instruction count ovvero $T_{CPU}$ è una $T_{CPU}(I(P))$), possiamo vedere questa dipendenza come\n$$\nN_{C}(P) = I(P) \\cdot CPI(P)\n$$\n\naverage clock cycles per instruction CPI (è una media di cicli per istruzione del programma P che esegue I(P) istruzioni) provides one way of comparing two different implementations of the identical instruction set architecture.\n$$\nT_{CPU}(P) = \\frac{I(P) \\cdot CPI(P)}{r_c}\n$$\n\nWe can measure the CPU execution time by running the program.\nclock cycle time is usually published instruction count and CPI can be more difficult to obtain instruction count by using software tools\nor by using a simulator of the architecture. Alternatively, hardware counters included in most processors, to record a variety of measurements. instruction count depends on the architecture, but not on the exact implementation\n\nCosa influenza $I$ e $CPI$?\n- if the **algorithm** uses more divides, it will tend to have a higher CPI;\n- statements in the **programming language** translated to processor instructions determine instruction count.\n- compiler\n- ISA (anche clock rate (?))\n\n\n##### Altro\nInvert CPI to talk about IPC instructions per clock cycle\n\ntoday’s processors can vary their clock rates Intel Core i7 will temporarily increase clock rate by about 10% until the chip gets too warm. Intel calls this Turbo mode.","x":2300,"y":1260,"width":660,"height":1200},
		{"id":"e4cd21ce4fcc8e29","type":"text","text":"# Power Efficiency\n\nclock rate and power are correlated. power provides a limit to what we can cool. energy metric joules is a better measure than a power rate like watts. integrated circuits CMOS (complementary metal oxide semiconductor energy consumption (is so-called dynamic energy) is consumed when transistors switch states from 0 to 1 and vice versa. dall'equazione dell'energia immagazzinata in u condensatore ricavo che power required per transistor\n$$\nP \\propto \\frac{1}{2} CV^2\\cdot f\n$$\n\ndove $f$ è la Frequency, function of the clock rate. C is function of number of transistors connected to an output (fanout (?????????)) and *technology* qualsiasi cosa voglia dire...\n\nEnergy and thus power can be reduced by lowering the voltage. In 20 years, voltages have gone from 5 V to 1 V. modern problem is that further lowering of the voltage appears to make the\ntransistors too leaky 40% of the power consumption in server chips is due to leakage.\nto address the power problem turn off parts of the chip that are not used in a given clock cycle.\nthere are many more expensive ways to cool chips, but generally too costly.\n\nleakage current flows even when a transistor is off 40% of the energy consumption increasing the number of transistors increases power dissipation hard to lower voltage further\n\nSunto: \n- power must be brought in and distributed around the chip\n - power is dissipated as heat and must be removed","x":2200,"y":2600,"width":817,"height":920},
		{"id":"78b3395d6c378f86","type":"file","file":"Capitolo 1/PowerAndClockRate.png","x":3240,"y":3120,"width":806,"height":340},
		{"id":"5ee91b04fb136a01","type":"text","text":"# Benchmark\n\nda capire se è una conseguenza del parallelismo (cioè se in uniprogramming sai predire le performance in modo preciso e qui no).\n\nto evaluate a new computer, set of benchmarks (programs specifically chosen to measure performance form a workload (A set of programs run on a computer) that the user hopes will predict the performance of the actual workload.\n\nSPEC (System Performance Evaluation Cooperative) creates standard sets of benchmarks for modern computer systems\n\nDividing the execution time of a reference processor by the execution time of the evaluated computer normalizes the execution time measurements. bigger numeric results indicate faster performance. SPECratio is inverse of execution time. A summary is obtained by taking the geometric mean of the SPECratios (why????) apply the geometric mean so that it gives the same relative answer no matter what computer is used to normalize the results (why???).\n\nQua lui fa la media geometrica di \"*execution time ratio$_i$*\". non sono sicuro di capire cosa significa...\n\nle pagine successive le definisco \"oscure\" (pag 48 del pdf)","x":1280,"y":2890,"width":817,"height":510},
		{"id":"a46ca09fa4d7ec52","type":"text","text":"## Parallelism\n\nthe programmer must divide an application so that each processor has roughly the same amount to do at the same time, and that the overhead of scheduling and coordination doesn’t fritter away the potential performance benefits of parallelism  schedule the sub-tasks balance the load.\n\nfalls short if conclusion (of operation) , couldn’t be written until all the other parts were completed. care must be taken to reduce communication and synchronization overhead.","x":1280,"y":2600,"width":817,"height":240},
		{"id":"a397c5d67ebfce21","type":"file","file":"Capitolo 1/BenchmarksSPEC.png","x":1377,"y":3460,"width":623,"height":300},
		{"id":"100ff872e0aa5955","type":"text","text":"execution time is the product of the three factors in this table: instruction count in billions, clocks per instruction (CPI), and clock cycle time in\nnanoseconds. SPECratio is simply the reference time, which is supplied by SPEC (di un processore di riferimento che non so quale sia, probabilente è vecchio), divided by the measured execution time. The single number\nquoted as SPECINTC2006 is the geometric mean of the SPECratios.","x":2080,"y":3640,"width":706,"height":193},
		{"id":"f9becf64c2d6e92f","type":"file","file":"Demetrescu/Ottimizzazione.txt","x":740,"y":2945,"width":400,"height":400},
		{"id":"35873d0873f6009a","type":"text","text":"il libro di tutto questo cita solo Amdahl ma fa sempre bene rileggere","x":815,"y":2720,"width":250,"height":120},
		{"id":"e71d455138a8e246","type":"text","text":"# Storia\n##### Incompleta, se proprio vuoi perdere tempo pagina 54 del pdf\n\nworld’s first\ncomputer.\nENIAC\nElectronic\nNumerical Integrator and Calculator\n\n1900 additions\nper second.\nprovided conditional jumps and was programmable\nmanually by\nplugging cables and setting switches\nvon Neumann helped crystallize the ideas and wrote a\nmemo\nbasis for the\nterm von Neumann compute\nEDSAC (Electronic Delay Storage Automatic\nCalculator)\n1949","x":3260,"y":1960,"width":441,"height":792},
		{"id":"20a4b8a4ba668db7","type":"text","text":"# (Riassunto) 2 - Linguaggio del Computer","x":-1900,"y":3220,"width":720,"height":100,"color":"6"},
		{"id":"e1323ed7fa9295aa","type":"text","text":"# Introduzione\n\ncomputer’s hardware vocabulary is called an instruction set. similarity of instruction sets occurs because all computers are constructed from hardware technologies based on similar underlying principles and because there are a few basic operations that all computers must provide.\n\nstored-program concept: idea that instructions and data of many types can be stored in memory as numbers\n\n","x":-1900,"y":3420,"width":720,"height":280},
		{"id":"48b5a89e36d2e078","type":"text","text":"questa roba è utilissima (ovviamente), sta un po' a cavallo tra i due capitoli","x":-1025,"y":2000,"width":250,"height":140},
		{"id":"c5d920fa7babd150","x":-1100,"y":2199,"width":400,"height":400,"type":"file","file":"Demetrescu/pipelining.txt"},
		{"id":"9e0df3cff264c0fd","type":"text","text":"# Nota sulle architetture\n\nil libro consigliato da lei dà le istruzioni in MIPS, quello che ho io usa RISC-V","x":-2344,"y":3171,"width":338,"height":198},
		{"id":"5b46158c20ea1d79","type":"file","file":"Capitolo 2/RISCvsMIPS.png","x":-2328,"y":2520,"width":307,"height":531},
		{"id":"bb9a38848c497ba0","type":"text","text":"### Quindi sono tipi di assembly come Intel e AT&T\nSì, RISC-V e MIPS sono entrambe architetture di set di istruzioni (ISA) che definiscono l'insieme di istruzioni disponibili per i processori che seguono quelle specifiche architetture. Queste istruzioni vengono eseguite dai processori come parte del loro lavoro di esecuzione di programmi software.\n\nL'assembly per RISC-V e MIPS è scritto utilizzando istruzioni specifiche per queste architetture. Ogni istruzione assembly corrisponde a un'operazione specifica che il processore deve eseguire, come ad esempio l'aggiunta di due numeri o il trasferimento di dati dalla memoria al registro del processore.\n\nSimilmente, Intel e AT&T sono due formati comuni per la scrittura di codice assembly per i processori x86 di Intel. Questi due formati differiscono nella loro sintassi e nella convenzione per la rappresentazione delle istruzioni, dei registri e degli operandi.\n\nQuindi, così come si ha il codice assembly specifico per i processori Intel scritto in formato Intel o AT&T, si ha anche il codice assembly specifico per le architetture RISC-V e MIPS scritto utilizzando le istruzioni e la sintassi specifiche di queste architetture.","x":-3040,"y":2480,"width":600,"height":611},
		{"id":"c84a18acd87bdb46","type":"file","file":"Capitolo 2/x86vsRISCvsMIPS.png","x":-3560,"y":2546,"width":418,"height":480},
		{"id":"3f126fb4a6f44890","type":"file","file":"Capitolo 2/ARMandOthers.png","x":-3980,"y":2546,"width":336,"height":480},
		{"id":"89901351b69f4ba9","type":"text","text":"# Commenti a caso che non so dove mettere, edizione capitolo 2\n\nDesign Principle 1: Simplicity favors regularity.\n\n","x":-1025,"y":2920,"width":720,"height":700},
		{"id":"78004fb19f7d1705","type":"text","text":"# RISC-V\n\nRISC-V assembly add a, b, c add the two variables b and c and to put their sum in a. each RISC-V arithmetic instruction performs only one operation and must always have exactly three variables each line of this language can contain at most one instruction. comments always terminate at the end of a line. every instruction to have exactly three operands philosophy of keeping the hardware simple.\n32 registri, memory word Accessed only by data transfer instructions, RISC-V uses byte addresses, so sequential doubleword *\\[Tipicamente per word si intendono 32 bit e doubleword 64, e questo è un commento che fa dopo e che mi sembra incoerente con questa frase\\]* accesses differ by 8\n###### ChatGPT:\nRISC-V non è limitato esclusivamente a 64-bit. L'architettura RISC-V è stata progettata per essere modulare e flessibile, il che significa che può supportare sia sistemi a 32-bit che a 64-bit, così come altre possibili larghezze di bus o parole.\n\nLa specifica base di RISC-V definisce istruzioni per un'architettura a 32-bit, mentre le estensioni possono essere utilizzate per supportare larghezze di bus e parole diverse, inclusi 64-bit. Questa flessibilità è una delle ragioni per cui RISC-V è popolare in una vasta gamma di applicazioni, dalle microcontroller embedded ai server ad alte prestazioni.","x":-2640,"y":3420,"width":634,"height":680},
		{"id":"41f1b1a58ee74373","x":-3460,"y":3405,"width":580,"height":710,"type":"text","text":"# Operandi dell'architettura: i registri\n\noperands must be from a limited number of special locations built directly in hardware called registers. size of a register in RISC-V is 64 bits (doubleword, 32 bits called a word)  limited number of registers, typically 32.\nDiktat: Smaller is faster\nlarge number of registers may increase the clock cycle time: electronic signals must travel farther. but 31 registers may not be\nfaster than 32. must balance the craving of programs for more registers with the designer’s desire to keep the clock cycle fast. \n\nRISC-V convention is *x* followed by the number of the register, except for a few register names.\n\nprocessor can keep only a small amount of data in registers. Arrays and structures are kept in memory RISC-V must include instructions that transfer data between memory and registers (**data transfer instruction**). must supply the memory address.\nMemory is just a large, single-dimensional array, with the address acting as the index to that array, starting at 0"}
	],
	"edges":[
		{"id":"352324112e64b113","fromNode":"3c12a472e21d0b51","fromSide":"right","toNode":"70f63fa924d39f03","toSide":"left"},
		{"id":"b9f09ab23c605f65","fromNode":"e723e0fbc1846d02","fromSide":"left","toNode":"3c12a472e21d0b51","toSide":"top"},
		{"id":"02479a35015b5cf2","fromNode":"70f63fa924d39f03","fromSide":"right","toNode":"3f606618d45d0104","toSide":"bottom"},
		{"id":"7698822a27692414","fromNode":"3f606618d45d0104","fromSide":"top","toNode":"7c44d0b7fa4661bf","toSide":"left"},
		{"id":"d750fa4a8a142824","fromNode":"969e5ce3a6049ee4","fromSide":"top","toNode":"6264c0d982a44972","toSide":"bottom"},
		{"id":"fc5ded6ec491d93d","fromNode":"7c44d0b7fa4661bf","fromSide":"right","toNode":"94ebb589ac92b6e9","toSide":"left"},
		{"id":"51251623d685e714","fromNode":"94ebb589ac92b6e9","fromSide":"right","toNode":"46ee857cb70eed15","toSide":"left"},
		{"id":"a7eafeb6c8c346f7","fromNode":"46ee857cb70eed15","fromSide":"right","toNode":"969e5ce3a6049ee4","toSide":"left"},
		{"id":"2812fd451c957750","fromNode":"94ebb589ac92b6e9","fromSide":"bottom","toNode":"7724895a9f26e30f","toSide":"top"},
		{"id":"f496ae47111d27c9","fromNode":"7724895a9f26e30f","fromSide":"right","toNode":"e4506d54dcf914ea","toSide":"left"},
		{"id":"99d0c42d87f2952f","fromNode":"e4506d54dcf914ea","fromSide":"right","toNode":"dfd2890763a0d962","toSide":"left"},
		{"id":"66d451b3dd1158ac","fromNode":"dfd2890763a0d962","fromSide":"bottom","toNode":"e4cd21ce4fcc8e29","toSide":"top"},
		{"id":"a195332ee9782da8","fromNode":"e4cd21ce4fcc8e29","fromSide":"right","toNode":"78b3395d6c378f86","toSide":"left"},
		{"id":"58858eb02c3845e0","fromNode":"a397c5d67ebfce21","fromSide":"right","toNode":"100ff872e0aa5955","toSide":"left"},
		{"id":"5c08edf34c14e11e","fromNode":"5ee91b04fb136a01","fromSide":"bottom","toNode":"a397c5d67ebfce21","toSide":"top"},
		{"id":"cb68dd135ac37fa6","fromNode":"e4cd21ce4fcc8e29","fromSide":"left","toNode":"a46ca09fa4d7ec52","toSide":"right"},
		{"id":"5a8f2417660fbab0","fromNode":"a46ca09fa4d7ec52","fromSide":"bottom","toNode":"5ee91b04fb136a01","toSide":"top"},
		{"id":"6977e7d4fb7b4066","fromNode":"5ee91b04fb136a01","fromSide":"left","toNode":"f9becf64c2d6e92f","toSide":"right"},
		{"id":"d5006d3c292b2549","fromNode":"f9becf64c2d6e92f","fromSide":"top","toNode":"35873d0873f6009a","toSide":"bottom"},
		{"id":"7c3340f8bc738f93","fromNode":"20a4b8a4ba668db7","fromSide":"left","toNode":"9e0df3cff264c0fd","toSide":"right"},
		{"id":"af64d4c35f73302e","fromNode":"9e0df3cff264c0fd","fromSide":"top","toNode":"5b46158c20ea1d79","toSide":"bottom"},
		{"id":"e7a9b3f05b7102f0","fromNode":"5b46158c20ea1d79","fromSide":"left","toNode":"bb9a38848c497ba0","toSide":"right"},
		{"id":"d419a393951d4d93","fromNode":"c84a18acd87bdb46","fromSide":"left","toNode":"3f126fb4a6f44890","toSide":"right"},
		{"id":"f9c00b563b03a0ea","fromNode":"bb9a38848c497ba0","fromSide":"left","toNode":"c84a18acd87bdb46","toSide":"right"},
		{"id":"01091e5356071f42","fromNode":"20a4b8a4ba668db7","fromSide":"right","toNode":"89901351b69f4ba9","toSide":"left"},
		{"id":"9b0641f0e1a9b609","fromNode":"20a4b8a4ba668db7","fromSide":"bottom","toNode":"e1323ed7fa9295aa","toSide":"top"},
		{"id":"1a68b05a1a14e06a","fromNode":"e1323ed7fa9295aa","fromSide":"left","toNode":"78004fb19f7d1705","toSide":"right"},
		{"id":"674c525131f49c68","fromNode":"78004fb19f7d1705","fromSide":"left","toNode":"41f1b1a58ee74373","toSide":"right"}
	]
}